{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcDesXsHSpus",
        "outputId": "058568f6-2911-4f40-ddb0-b4a3aafb4604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame head after converting 'date' column:\n",
            "        date  temperature  humidity  irradiance  potential  wind_speed\n",
            "0 1950-01-01       20.300    73.536      637.50     4.3901     0.23750\n",
            "1 1950-01-02       20.352    73.225      837.50     4.2901     0.26750\n",
            "2 1950-01-03       20.654    75.678      791.67     4.0553     0.47449\n",
            "3 1950-01-04       21.388    73.610      850.00     4.3541     0.44123\n",
            "4 1950-01-05       21.435    74.990      841.67     4.3115     0.35958\n",
            "\n",
            "DataFrame info after converting 'date' column:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27214 entries, 0 to 27213\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   date         27214 non-null  datetime64[ns]\n",
            " 1   temperature  27214 non-null  float64       \n",
            " 2   humidity     27214 non-null  float64       \n",
            " 3   irradiance   27214 non-null  float64       \n",
            " 4   potential    27214 non-null  float64       \n",
            " 5   wind_speed   27214 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(5)\n",
            "memory usage: 1.2 MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Bambili_treated.csv file\n",
        "file_name = \"Bambili_treated.csv\"\n",
        "bambili_df = pd.read_csv(file_name)\n",
        "\n",
        "# Convert the 'date' column to datetime objects\n",
        "# The format '%Y%m%d' specifies that dates are like 'YYYYMMDD' (e.g., 19500101)\n",
        "bambili_df['date'] = pd.to_datetime(bambili_df['date'], format='%Y%m%d')\n",
        "\n",
        "print(\"DataFrame head after converting 'date' column:\")\n",
        "print(bambili_df.head())\n",
        "print(\"\\nDataFrame info after converting 'date' column:\")\n",
        "print(bambili_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract various time-based features\n",
        "bambili_df['year'] = bambili_df['date'].dt.year\n",
        "bambili_df['month'] = bambili_df['date'].dt.month\n",
        "bambili_df['day_of_month'] = bambili_df['date'].dt.day\n",
        "bambili_df['day_of_week'] = bambili_df['date'].dt.dayofweek # Monday=0, Sunday=6\n",
        "bambili_df['day_of_year'] = bambili_df['date'].dt.dayofyear\n",
        "bambili_df['week_of_year'] = bambili_df['date'].dt.isocalendar().week.astype(int)\n",
        "bambili_df['quarter'] = bambili_df['date'].dt.quarter\n",
        "\n",
        "# Add binary flags for start/end of month, quarter, and year\n",
        "bambili_df['is_month_start'] = bambili_df['date'].dt.is_month_start.astype(int)\n",
        "bambili_df['is_month_end'] = bambili_df['date'].dt.is_month_end.astype(int)\n",
        "bambili_df['is_quarter_start'] = bambili_df['date'].dt.is_quarter_start.astype(int)\n",
        "bambili_df['is_quarter_end'] = bambili_df['date'].dt.is_quarter_end.astype(int)\n",
        "bambili_df['is_year_start'] = bambili_df['date'].dt.is_year_start.astype(int)\n",
        "bambili_df['is_year_end'] = bambili_df['date'].dt.is_year_end.astype(int)\n",
        "\n",
        "# Add cyclical features for Month, Day of Year, and Day of Week\n",
        "# Month (1-12)\n",
        "bambili_df['month_sin'] = np.sin(2 * np.pi * bambili_df['month'] / 12)\n",
        "bambili_df['month_cos'] = np.cos(2 * np.pi * bambili_df['month'] / 12)\n",
        "\n",
        "# Day of Year (1-366 for leap years)\n",
        "# Use 366 for the cycle to account for leap years, or 365.25 for average\n",
        "bambili_df['day_of_year_sin'] = np.sin(2 * np.pi * bambili_df['day_of_year'] / 366)\n",
        "bambili_df['day_of_year_cos'] = np.cos(2 * np.pi * bambili_df['day_of_year'] / 366)\n",
        "\n",
        "# Day of Week (0-6)\n",
        "bambili_df['day_of_week_sin'] = np.sin(2 * np.pi * bambili_df['day_of_week'] / 7)\n",
        "bambili_df['day_of_week_cos'] = np.cos(2 * np.pi * bambili_df['day_of_week'] / 7)\n",
        "\n",
        "# Display the first few rows with new features and updated info\n",
        "print(\"\\nDataFrame head with new time-based features:\")\n",
        "print(bambili_df.head())\n",
        "print(\"\\nDataFrame info with new time-based features:\")\n",
        "print(bambili_df.info())\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_name = \"Bambili_treated_with_time_features.csv\"\n",
        "bambili_df.to_csv(output_file_name, index=False)\n",
        "print(f\"\\nUpdated dataset saved to '{output_file_name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wdys97sTxWX",
        "outputId": "1e9715a0-621c-49f8-8f1b-c63983d3d777"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame head with new time-based features:\n",
            "        date  temperature  humidity  irradiance  potential  wind_speed  year  \\\n",
            "0 1950-01-01       20.300    73.536      637.50     4.3901     0.23750  1950   \n",
            "1 1950-01-02       20.352    73.225      837.50     4.2901     0.26750  1950   \n",
            "2 1950-01-03       20.654    75.678      791.67     4.0553     0.47449  1950   \n",
            "3 1950-01-04       21.388    73.610      850.00     4.3541     0.44123  1950   \n",
            "4 1950-01-05       21.435    74.990      841.67     4.3115     0.35958  1950   \n",
            "\n",
            "   month  day_of_month  day_of_week  ...  is_quarter_start  is_quarter_end  \\\n",
            "0      1             1            6  ...                 1               0   \n",
            "1      1             2            0  ...                 0               0   \n",
            "2      1             3            1  ...                 0               0   \n",
            "3      1             4            2  ...                 0               0   \n",
            "4      1             5            3  ...                 0               0   \n",
            "\n",
            "   is_year_start  is_year_end  month_sin  month_cos  day_of_year_sin  \\\n",
            "0              1            0        0.5   0.866025         0.017166   \n",
            "1              0            0        0.5   0.866025         0.034328   \n",
            "2              0            0        0.5   0.866025         0.051479   \n",
            "3              0            0        0.5   0.866025         0.068615   \n",
            "4              0            0        0.5   0.866025         0.085731   \n",
            "\n",
            "   day_of_year_cos  day_of_week_sin  day_of_week_cos  \n",
            "0         0.999853        -0.781831         0.623490  \n",
            "1         0.999411         0.000000         1.000000  \n",
            "2         0.998674         0.781831         0.623490  \n",
            "3         0.997643         0.974928        -0.222521  \n",
            "4         0.996318         0.433884        -0.900969  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "\n",
            "DataFrame info with new time-based features:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27214 entries, 0 to 27213\n",
            "Data columns (total 25 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   date              27214 non-null  datetime64[ns]\n",
            " 1   temperature       27214 non-null  float64       \n",
            " 2   humidity          27214 non-null  float64       \n",
            " 3   irradiance        27214 non-null  float64       \n",
            " 4   potential         27214 non-null  float64       \n",
            " 5   wind_speed        27214 non-null  float64       \n",
            " 6   year              27214 non-null  int32         \n",
            " 7   month             27214 non-null  int32         \n",
            " 8   day_of_month      27214 non-null  int32         \n",
            " 9   day_of_week       27214 non-null  int32         \n",
            " 10  day_of_year       27214 non-null  int32         \n",
            " 11  week_of_year      27214 non-null  int64         \n",
            " 12  quarter           27214 non-null  int32         \n",
            " 13  is_month_start    27214 non-null  int64         \n",
            " 14  is_month_end      27214 non-null  int64         \n",
            " 15  is_quarter_start  27214 non-null  int64         \n",
            " 16  is_quarter_end    27214 non-null  int64         \n",
            " 17  is_year_start     27214 non-null  int64         \n",
            " 18  is_year_end       27214 non-null  int64         \n",
            " 19  month_sin         27214 non-null  float64       \n",
            " 20  month_cos         27214 non-null  float64       \n",
            " 21  day_of_year_sin   27214 non-null  float64       \n",
            " 22  day_of_year_cos   27214 non-null  float64       \n",
            " 23  day_of_week_sin   27214 non-null  float64       \n",
            " 24  day_of_week_cos   27214 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(11), int32(6), int64(7)\n",
            "memory usage: 4.6 MB\n",
            "None\n",
            "\n",
            "Updated dataset saved to 'Bambili_treated_with_time_features.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Load the updated Bambili dataset with time-based features\n",
        "file_name = \"Bambili_treated_with_time_features.csv\"\n",
        "bambili_df_features = pd.read_csv(file_name)\n",
        "\n",
        "# Display the first few rows and information about the DataFrame to confirm the new features\n",
        "print(\"Bambili_treated_with_time_features.csv head:\")\n",
        "print(bambili_df_features.head())\n",
        "print(\"\\nBambili_treated_with_time_features.csv info:\")\n",
        "print(bambili_df_features.info())\n",
        "\n",
        "# Define the target variable\n",
        "target_column = 'irradiance'\n",
        "\n",
        "# Define features (X) by dropping the 'date' column and the target column\n",
        "# The 'date' column itself is no longer needed as its information is now in the new features.\n",
        "X_bambili = bambili_df_features.drop(columns=['date', target_column])\n",
        "y_bambili = bambili_df_features[target_column]\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "estimator = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Initialize RFE with n_features_to_select = 14\n",
        "# The 'step' parameter controls how many features are removed at each iteration.\n",
        "# A step of 1 means one feature is removed at a time.\n",
        "selector = RFE(estimator=estimator, n_features_to_select=14, step=1)\n",
        "\n",
        "# Fit RFE to the Bambili data with the expanded feature set\n",
        "selector.fit(X_bambili, y_bambili)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features_mask = selector.support_\n",
        "selected_feature_names = X_bambili.columns[selected_features_mask].tolist()\n",
        "\n",
        "print(f\"\\nOriginal number of features (excluding 'date' and target): {X_bambili.shape[1]}\")\n",
        "print(f\"Selected 14 features for Bambili using RFE: {selected_feature_names}\")\n",
        "\n",
        "# Optionally, you can also get the ranking of features (1 being the most important)\n",
        "feature_ranking = pd.DataFrame({'Feature': X_bambili.columns, 'Ranking': selector.ranking_})\n",
        "feature_ranking = feature_ranking.sort_values(by='Ranking')\n",
        "print(\"\\nFeature ranking (1 = selected/most important):\")\n",
        "print(feature_ranking)\n",
        "\n",
        "# Create a DataFrame with only the selected features and the target variable\n",
        "selected_df = bambili_df_features[selected_feature_names + [target_column]]\n",
        "\n",
        "# Save the DataFrame with selected features\n",
        "output_file_name_selected = \"Bambili_selected_features.csv\"\n",
        "selected_df.to_csv(output_file_name_selected, index=False)\n",
        "print(f\"\\nDataset with selected features saved to '{output_file_name_selected}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOflSICGT1e7",
        "outputId": "ac9cb8a8-dd4c-4f36-c4de-e514c78ca12e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bambili_treated_with_time_features.csv head:\n",
            "         date  temperature  humidity  irradiance  potential  wind_speed  year  \\\n",
            "0  1950-01-01       20.300    73.536      637.50     4.3901     0.23750  1950   \n",
            "1  1950-01-02       20.352    73.225      837.50     4.2901     0.26750  1950   \n",
            "2  1950-01-03       20.654    75.678      791.67     4.0553     0.47449  1950   \n",
            "3  1950-01-04       21.388    73.610      850.00     4.3541     0.44123  1950   \n",
            "4  1950-01-05       21.435    74.990      841.67     4.3115     0.35958  1950   \n",
            "\n",
            "   month  day_of_month  day_of_week  ...  is_quarter_start  is_quarter_end  \\\n",
            "0      1             1            6  ...                 1               0   \n",
            "1      1             2            0  ...                 0               0   \n",
            "2      1             3            1  ...                 0               0   \n",
            "3      1             4            2  ...                 0               0   \n",
            "4      1             5            3  ...                 0               0   \n",
            "\n",
            "   is_year_start  is_year_end  month_sin  month_cos  day_of_year_sin  \\\n",
            "0              1            0        0.5   0.866025         0.017166   \n",
            "1              0            0        0.5   0.866025         0.034328   \n",
            "2              0            0        0.5   0.866025         0.051479   \n",
            "3              0            0        0.5   0.866025         0.068615   \n",
            "4              0            0        0.5   0.866025         0.085731   \n",
            "\n",
            "   day_of_year_cos  day_of_week_sin  day_of_week_cos  \n",
            "0         0.999853        -0.781831         0.623490  \n",
            "1         0.999411         0.000000         1.000000  \n",
            "2         0.998674         0.781831         0.623490  \n",
            "3         0.997643         0.974928        -0.222521  \n",
            "4         0.996318         0.433884        -0.900969  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "\n",
            "Bambili_treated_with_time_features.csv info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27214 entries, 0 to 27213\n",
            "Data columns (total 25 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   date              27214 non-null  object \n",
            " 1   temperature       27214 non-null  float64\n",
            " 2   humidity          27214 non-null  float64\n",
            " 3   irradiance        27214 non-null  float64\n",
            " 4   potential         27214 non-null  float64\n",
            " 5   wind_speed        27214 non-null  float64\n",
            " 6   year              27214 non-null  int64  \n",
            " 7   month             27214 non-null  int64  \n",
            " 8   day_of_month      27214 non-null  int64  \n",
            " 9   day_of_week       27214 non-null  int64  \n",
            " 10  day_of_year       27214 non-null  int64  \n",
            " 11  week_of_year      27214 non-null  int64  \n",
            " 12  quarter           27214 non-null  int64  \n",
            " 13  is_month_start    27214 non-null  int64  \n",
            " 14  is_month_end      27214 non-null  int64  \n",
            " 15  is_quarter_start  27214 non-null  int64  \n",
            " 16  is_quarter_end    27214 non-null  int64  \n",
            " 17  is_year_start     27214 non-null  int64  \n",
            " 18  is_year_end       27214 non-null  int64  \n",
            " 19  month_sin         27214 non-null  float64\n",
            " 20  month_cos         27214 non-null  float64\n",
            " 21  day_of_year_sin   27214 non-null  float64\n",
            " 22  day_of_year_cos   27214 non-null  float64\n",
            " 23  day_of_week_sin   27214 non-null  float64\n",
            " 24  day_of_week_cos   27214 non-null  float64\n",
            "dtypes: float64(11), int64(13), object(1)\n",
            "memory usage: 5.2+ MB\n",
            "None\n",
            "\n",
            "Original number of features (excluding 'date' and target): 23\n",
            "Selected 14 features for Bambili using RFE: ['temperature', 'humidity', 'potential', 'wind_speed', 'year', 'day_of_month', 'day_of_week', 'day_of_year', 'week_of_year', 'month_sin', 'day_of_year_sin', 'day_of_year_cos', 'day_of_week_sin', 'day_of_week_cos']\n",
            "\n",
            "Feature ranking (1 = selected/most important):\n",
            "             Feature  Ranking\n",
            "0        temperature        1\n",
            "1           humidity        1\n",
            "2          potential        1\n",
            "3         wind_speed        1\n",
            "4               year        1\n",
            "6       day_of_month        1\n",
            "7        day_of_week        1\n",
            "8        day_of_year        1\n",
            "9       week_of_year        1\n",
            "21   day_of_week_sin        1\n",
            "19   day_of_year_sin        1\n",
            "22   day_of_week_cos        1\n",
            "20   day_of_year_cos        1\n",
            "17         month_sin        1\n",
            "5              month        2\n",
            "18         month_cos        3\n",
            "10           quarter        4\n",
            "11    is_month_start        5\n",
            "12      is_month_end        6\n",
            "13  is_quarter_start        7\n",
            "14    is_quarter_end        8\n",
            "15     is_year_start        9\n",
            "16       is_year_end       10\n",
            "\n",
            "Dataset with selected features saved to 'Bambili_selected_features.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the dataset with selected features\n",
        "file_name = \"Bambili_selected_features.csv\"\n",
        "bambili_df_selected = pd.read_csv(file_name)\n",
        "\n",
        "print(\"DataFrame head with selected features:\")\n",
        "print(bambili_df_selected.head())\n",
        "print(\"\\nDataFrame info with selected features:\")\n",
        "print(bambili_df_selected.info())\n",
        "\n",
        "# Define the target variable\n",
        "target_column = 'irradiance'\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = bambili_df_selected.drop(columns=[target_column])\n",
        "y = bambili_df_selected[target_column]\n",
        "\n",
        "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "# Prepare for 10-fold cross-validation\n",
        "# The paper mentions \"tenfold cross validation techniques\" for model development and evaluation.\n",
        "# KFold is a good way to generate the indices for these folds.\n",
        "n_splits = 10 # As per \"tenfold cross validation\"\n",
        "\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nPreparing for {n_splits}-fold cross-validation:\")\n",
        "print(\"Iterating through folds to show the split indices:\")\n",
        "\n",
        "fold_count = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    fold_count += 1\n",
        "    # You would typically use these indices to get your train and test sets for each fold\n",
        "    # X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    # y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    print(f\"  Fold {fold_count}:\")\n",
        "    print(f\"    Train set size: {len(train_index)} samples\")\n",
        "    print(f\"    Test set size: {len(test_index)} samples\")\n",
        "    # For brevity, we are not printing the actual data, just the sizes.\n",
        "    if fold_count >= 3: # Print only first 3 folds to avoid too much output\n",
        "        print(\"  ...\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nData successfully prepared for {n_splits}-fold cross-validation.\")\n",
        "print(\"Each iteration of the KFold object provides indices to split your data into training and testing sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJw4UCzjU-Hs",
        "outputId": "138ed620-634d-405e-b142-5dceb924fb1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame head with selected features:\n",
            "   temperature  humidity  potential  wind_speed  year  day_of_month  \\\n",
            "0       20.300    73.536     4.3901     0.23750  1950             1   \n",
            "1       20.352    73.225     4.2901     0.26750  1950             2   \n",
            "2       20.654    75.678     4.0553     0.47449  1950             3   \n",
            "3       21.388    73.610     4.3541     0.44123  1950             4   \n",
            "4       21.435    74.990     4.3115     0.35958  1950             5   \n",
            "\n",
            "   day_of_week  day_of_year  week_of_year  month_sin  day_of_year_sin  \\\n",
            "0            6            1            52        0.5         0.017166   \n",
            "1            0            2             1        0.5         0.034328   \n",
            "2            1            3             1        0.5         0.051479   \n",
            "3            2            4             1        0.5         0.068615   \n",
            "4            3            5             1        0.5         0.085731   \n",
            "\n",
            "   day_of_year_cos  day_of_week_sin  day_of_week_cos  irradiance  \n",
            "0         0.999853        -0.781831         0.623490      637.50  \n",
            "1         0.999411         0.000000         1.000000      837.50  \n",
            "2         0.998674         0.781831         0.623490      791.67  \n",
            "3         0.997643         0.974928        -0.222521      850.00  \n",
            "4         0.996318         0.433884        -0.900969      841.67  \n",
            "\n",
            "DataFrame info with selected features:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27214 entries, 0 to 27213\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   temperature      27214 non-null  float64\n",
            " 1   humidity         27214 non-null  float64\n",
            " 2   potential        27214 non-null  float64\n",
            " 3   wind_speed       27214 non-null  float64\n",
            " 4   year             27214 non-null  int64  \n",
            " 5   day_of_month     27214 non-null  int64  \n",
            " 6   day_of_week      27214 non-null  int64  \n",
            " 7   day_of_year      27214 non-null  int64  \n",
            " 8   week_of_year     27214 non-null  int64  \n",
            " 9   month_sin        27214 non-null  float64\n",
            " 10  day_of_year_sin  27214 non-null  float64\n",
            " 11  day_of_year_cos  27214 non-null  float64\n",
            " 12  day_of_week_sin  27214 non-null  float64\n",
            " 13  day_of_week_cos  27214 non-null  float64\n",
            " 14  irradiance       27214 non-null  float64\n",
            "dtypes: float64(10), int64(5)\n",
            "memory usage: 3.1 MB\n",
            "None\n",
            "\n",
            "Features (X) shape: (27214, 14)\n",
            "Target (y) shape: (27214,)\n",
            "\n",
            "Preparing for 10-fold cross-validation:\n",
            "Iterating through folds to show the split indices:\n",
            "  Fold 1:\n",
            "    Train set size: 24492 samples\n",
            "    Test set size: 2722 samples\n",
            "  Fold 2:\n",
            "    Train set size: 24492 samples\n",
            "    Test set size: 2722 samples\n",
            "  Fold 3:\n",
            "    Train set size: 24492 samples\n",
            "    Test set size: 2722 samples\n",
            "  ...\n",
            "\n",
            "Data successfully prepared for 10-fold cross-validation.\n",
            "Each iteration of the KFold object provides indices to split your data into training and testing sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the dataset with selected features\n",
        "file_name = \"Bambili_selected_features.csv\"\n",
        "bambili_df_selected = pd.read_csv(file_name)\n",
        "\n",
        "# Define the target variable\n",
        "target_column = 'irradiance'\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = bambili_df_selected.drop(columns=[target_column])\n",
        "y = bambili_df_selected[target_column]\n",
        "\n",
        "# Normalize features and target (important for neural networks)\n",
        "# Use MinMaxScaler for features (X) and target (y) separately\n",
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "# Reshape y to 2D array for scaler (it expects 2D input)\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "print(f\"X_scaled shape: {X_scaled.shape}\")\n",
        "print(f\"y_scaled shape: {y_scaled.shape}\")\n",
        "\n",
        "# Function to create sequences for CNN-LSTM\n",
        "# n_timesteps: how many past days/observations to use for prediction\n",
        "def create_sequences(X, y, n_timesteps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - n_timesteps):\n",
        "        # Features from t to t + n_timesteps - 1\n",
        "        Xs.append(X[i:(i + n_timesteps)])\n",
        "        # Target at t + n_timesteps\n",
        "        ys.append(y[i + n_timesteps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# Define the number of timesteps. This is a hyperparameter!\n",
        "# Let's start with 7 timesteps (using the past 7 days to predict the next day).\n",
        "n_timesteps = 7\n",
        "print(f\"\\nCreating sequences with {n_timesteps} timesteps...\")\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, n_timesteps)\n",
        "\n",
        "print(f\"Shape of X_seq (samples, timesteps, features): {X_seq.shape}\")\n",
        "print(f\"Shape of y_seq (samples, target): {y_seq.shape}\")\n",
        "\n",
        "# Prepare for 10-fold cross-validation (using the sequential data)\n",
        "n_splits = 10\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# This loop will now be used for actual model training in the next steps\n",
        "# print(f\"\\nSample KFold split with sequential data:\")\n",
        "# fold_count = 0\n",
        "# for train_index, test_index in kf.split(X_seq):\n",
        "#     fold_count += 1\n",
        "#     print(f\"  Fold {fold_count}: Train samples: {len(train_index)}, Test samples: {len(test_index)}\")\n",
        "#     if fold_count >= 1: # Just show one example\n",
        "#         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYdEnlVSXhrF",
        "outputId": "196056b4-f35b-4a6b-8584-3984e706d837"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_scaled shape: (27214, 14)\n",
            "y_scaled shape: (27214, 1)\n",
            "\n",
            "Creating sequences with 7 timesteps...\n",
            "Shape of X_seq (samples, timesteps, features): (27207, 7, 14)\n",
            "Shape of y_seq (samples, target): (27207, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam # A common optimizer\n",
        "\n",
        "# --- Model Definition ---\n",
        "\n",
        "# Define the number of features (columns in X_seq, which is the last dimension of X_seq.shape)\n",
        "n_features = X_seq.shape[2] # Should be 14\n",
        "\n",
        "# Define the CNN-SLSTM model\n",
        "def build_cnn_slstm_model(n_timesteps, n_features,\n",
        "                          filters=64, kernel_size=2, pool_size=2,\n",
        "                          lstm_units_1=50, lstm_units_2=50,\n",
        "                          dense_units_1=25, dense_units_2=10): # Added a second dense layer as per description\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # CNN Part\n",
        "    # Conv1D processes sequences (timesteps, features)\n",
        "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
        "                     input_shape=(n_timesteps, n_features)))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    # Flatten the output of the CNN to feed into the LSTM.\n",
        "    # The LSTM expects a 2D input (samples, features) after processing a sequence,\n",
        "    # or a 3D input if it processes a sequence of sequences.\n",
        "    # Given Conv1D and MaxPooling reduce the timesteps, a flatten is typical if LSTM follows\n",
        "    # and processes the reduced sequence as a feature vector per sample.\n",
        "    # However, if LSTM is meant to continue processing a sequence, the Conv1D's output\n",
        "    # (after pooling) should be directly fed without flatten if its output shape is (batch, new_timesteps, filters).\n",
        "    # Let's assume the CNN is feature extraction over the sequence, and the LSTM operates on these processed sequences.\n",
        "    # If MaxPooling1D significantly reduces timesteps, the LSTM might expect the output of CNN to still be sequential.\n",
        "    # A common design is to feed the output of MaxPooling1D directly to LSTM.\n",
        "    # Let's verify the shape transition:\n",
        "    # (None, n_timesteps, n_features) -> Conv1D -> (None, n_timesteps - kernel_size + 1, filters) -> MaxPooling1D -> (None, (n_timesteps - kernel_size + 1) // pool_size, filters)\n",
        "    # This shape is compatible with LSTM.\n",
        "\n",
        "    # Stacked LSTM Part\n",
        "    # First LSTM layer: return_sequences=True to pass the sequence to the next LSTM layer\n",
        "    model.add(LSTM(units=lstm_units_1, activation='relu', return_sequences=True))\n",
        "    # Second LSTM layer: No return_sequences=True as it's the last LSTM before dense layers\n",
        "    model.add(LSTM(units=lstm_units_2, activation='relu'))\n",
        "\n",
        "    # Dense (Fully Connected) Layers as per paper (two dense layers)\n",
        "    model.add(Dense(units=dense_units_1, activation='relu'))\n",
        "    model.add(Dense(units=dense_units_2, activation='relu')) # Added a second dense layer\n",
        "\n",
        "    # Output Layer for Regression\n",
        "    model.add(Dense(units=1, activation='linear')) # Linear activation for regression output\n",
        "\n",
        "    # Compile the model\n",
        "    # 'optimizer': Adam is a popular choice.\n",
        "    # 'loss': 'mse' (Mean Squared Error) is common for regression tasks.\n",
        "    model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build an instance of the model to see its summary\n",
        "# These are placeholder values for hyperparameters, which SMO would optimize later.\n",
        "model = build_cnn_slstm_model(n_timesteps=n_timesteps, n_features=n_features,\n",
        "                              filters=64, kernel_size=2, pool_size=2,\n",
        "                              lstm_units_1=50, lstm_units_2=50,\n",
        "                              dense_units_1=25, dense_units_2=10)\n",
        "\n",
        "print(\"\\n--- CNN-SLSTM Model Summary ---\")\n",
        "model.summary()\n",
        "\n",
        "# You can save the model architecture for later if needed (optional)\n",
        "# model.save('cnn_slstm_architecture.h5') # Saves structure, weights, and optimizer state\n",
        "print(\"\\nCNN-SLSTM model architecture defined successfully.\")\n",
        "print(\"The next step would involve training this model using the K-Fold splits and optimizing hyperparameters with SMO.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "08lKKd1lXpgl",
        "outputId": "36fed1e6-e294-4652-ef2a-130a8e04ede3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CNN-SLSTM Model Summary ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m23,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m260\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,602\u001b[0m (182.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,602</span> (182.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,602\u001b[0m (182.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,602</span> (182.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN-SLSTM model architecture defined successfully.\n",
            "The next step would involve training this model using the K-Fold splits and optimizing hyperparameters with SMO.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSQ_9w3iX2H7",
        "outputId": "badd8e63-77a9-4870-c63f-bd018d886bdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.5.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.5.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import Bayesian Optimization library\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output during optimization\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# --- 1. Data Preparation ---\n",
        "# Load the dataset with selected features\n",
        "file_name = \"Bambili_selected_features.csv\"\n",
        "bambili_df_selected = pd.read_csv(file_name)\n",
        "\n",
        "# Define the target variable\n",
        "target_column = 'irradiance'\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X_raw = bambili_df_selected.drop(columns=[target_column])\n",
        "y_raw = bambili_df_selected[target_column]\n",
        "\n",
        "# Normalize features and target\n",
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_raw)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_scaled = scaler_y.fit_transform(y_raw.values.reshape(-1, 1))\n",
        "\n",
        "# Function to create sequences for CNN-LSTM\n",
        "def create_sequences(X, y, n_timesteps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - n_timesteps):\n",
        "        Xs.append(X[i:(i + n_timesteps)])\n",
        "        ys.append(y[i + n_timesteps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# Get the number of features (columns in X_raw)\n",
        "n_features = X_raw.shape[1]\n",
        "\n",
        "# --- 2. CNN-SLSTM Model Definition Function ---\n",
        "def build_cnn_slstm_model(n_timesteps_param, n_features_param,\n",
        "                          filters_param, kernel_size_param,\n",
        "                          lstm_units_1_param, lstm_units_2_param,\n",
        "                          dense_units_1_param, dense_units_2_param,\n",
        "                          learning_rate_param, fixed_pool_size=2):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=filters_param, kernel_size=(kernel_size_param,), activation='relu',\n",
        "                     input_shape=(n_timesteps_param, n_features_param)))\n",
        "    model.add(MaxPooling1D(pool_size=fixed_pool_size))\n",
        "    model.add(LSTM(units=int(lstm_units_1_param), activation='relu', return_sequences=True))\n",
        "    model.add(LSTM(units=int(lstm_units_2_param), activation='relu'))\n",
        "    model.add(Dense(units=dense_units_1_param, activation='relu'))\n",
        "    model.add(Dense(units=dense_units_2_param, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='linear')) # Output layer for regression\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate_param)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# --- 3. Define the Search Space for Hyperparameters ---\n",
        "space = [\n",
        "    Integer(3, 10, name='n_timesteps'),        # Number of past days to consider (sequence length)\n",
        "    Integer(32, 128, name='filters'),          # Number of filters in Conv1D\n",
        "    Integer(2, 3, name='kernel_size'),         # Kernel size for Conv1D\n",
        "    Integer(32, 100, name='lstm_units_1'),     # Units in first LSTM layer\n",
        "    Integer(32, 100, name='lstm_units_2'),     # Units in second LSTM layer\n",
        "    Integer(10, 50, name='dense_units_1'),     # Units in first Dense layer\n",
        "    Integer(5, 20, name='dense_units_2'),      # Units in second Dense layer\n",
        "    Real(1e-4, 1e-2, \"log-uniform\", name='learning_rate'), # Adam optimizer learning rate\n",
        "    Integer(50, 200, name='epochs'),           # Number of training epochs per fold\n",
        "    Integer(16, 64, name='batch_size', prior='log-uniform') # Batch size for training (often powers of 2)\n",
        "]\n",
        "\n",
        "# --- 4. Objective Function for Bayesian Optimization ---\n",
        "@use_named_args(space)\n",
        "def objective_function(**hyperparameters):\n",
        "    # Extract hyperparameters from the input dictionary\n",
        "    n_timesteps = hyperparameters['n_timesteps']\n",
        "    filters = hyperparameters['filters']\n",
        "    kernel_size = hyperparameters['kernel_size']\n",
        "    lstm_units_1 = hyperparameters['lstm_units_1']\n",
        "    lstm_units_2 = hyperparameters['lstm_units_2']\n",
        "    dense_units_1 = hyperparameters['dense_units_1']\n",
        "    dense_units_2 = hyperparameters['dense_units_2']\n",
        "    learning_rate = hyperparameters['learning_rate']\n",
        "    epochs = hyperparameters['epochs']\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "\n",
        "    # Fixed pool_size value\n",
        "    fixed_pool_size = 2\n",
        "\n",
        "    print(f\"\\n--- Evaluating Trial ---\")\n",
        "    print(f\"Hyperparams: n_timesteps={n_timesteps}, filters={filters}, kernel_size={kernel_size}, \"\n",
        "          f\"pool_size={fixed_pool_size}, lstm_units_1={lstm_units_1}, lstm_units_2={lstm_units_2}, \"\n",
        "          f\"dense_units_1={dense_units_1}, dense_units_2={dense_units_2}, \"\n",
        "          f\"lr={learning_rate:.6f}, epochs={epochs}, batch_size={batch_size}\")\n",
        "\n",
        "    # --- NEW CHECK: Ensure Conv1D output is large enough for MaxPooling1D ---\n",
        "    conv1d_output_length = n_timesteps - kernel_size + 1\n",
        "    if conv1d_output_length < fixed_pool_size:\n",
        "        print(f\"  SKIPPING: Invalid combination (n_timesteps={n_timesteps}, kernel_size={kernel_size}). \"\n",
        "              f\"Conv1D output length ({conv1d_output_length}) is too small for MaxPooling1D (pool_size={fixed_pool_size}).\")\n",
        "        return 1e9 # Penalize this invalid combination with a very high error\n",
        "\n",
        "    # Re-create sequences based on the current n_timesteps hyperparameter\n",
        "    try:\n",
        "        current_X_seq, current_y_seq = create_sequences(X_scaled, y_scaled, n_timesteps)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error creating sequences for n_timesteps={n_timesteps}: {e}\")\n",
        "        return 1e9 # Return a very high error to penalize invalid n_timesteps values\n",
        "\n",
        "    if current_X_seq.shape[0] == 0: # Handle cases where n_timesteps is too large for data\n",
        "        print(f\"Not enough data for n_timesteps={n_timesteps}. Returning high error.\")\n",
        "        return 1e9\n",
        "\n",
        "    # Prepare for 10-fold cross-validation\n",
        "    n_splits = 10\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_mses = [] # To store MSE for each fold\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(current_X_seq)):\n",
        "        # Clear Keras session for each fold to prevent memory issues and model state leakage\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        # Build the model with the current hyperparameters and fixed pool_size\n",
        "        model = build_cnn_slstm_model(n_timesteps_param=n_timesteps,\n",
        "                                      n_features_param=n_features,\n",
        "                                      filters_param=filters,\n",
        "                                      kernel_size_param=kernel_size,\n",
        "                                      lstm_units_1_param=lstm_units_1,\n",
        "                                      lstm_units_2_param=lstm_units_2,\n",
        "                                      dense_units_1_param=dense_units_1,\n",
        "                                      dense_units_2_param=dense_units_2,\n",
        "                                      learning_rate_param=learning_rate,\n",
        "                                      fixed_pool_size=fixed_pool_size) # Pass fixed pool_size\n",
        "\n",
        "        # Split data for the current fold\n",
        "        X_train, X_val = current_X_seq[train_index], current_X_seq[val_index]\n",
        "        y_train, y_val = current_y_seq[train_index], current_y_seq[val_index]\n",
        "\n",
        "        # Fit the model for the current fold\n",
        "        try:\n",
        "            model.fit(X_train, y_train,\n",
        "                      epochs=epochs,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      verbose=0) # Set verbose to 0 to suppress training output per epoch\n",
        "\n",
        "            # Evaluate the model on the validation set of the current fold\n",
        "            mse = model.evaluate(X_val, y_val, verbose=0)\n",
        "            fold_mses.append(mse)\n",
        "            print(f\"  Fold {fold+1} MSE: {mse:.4f}\")\n",
        "        except tf.errors.InvalidArgumentError as e:\n",
        "            print(f\"TensorFlow Invalid Argument Error during training/evaluation in fold {fold+1}: {e}\")\n",
        "            fold_mses.append(1e9) # Penalize with high error\n",
        "        except Exception as e:\n",
        "            print(f\"General error during training/evaluation in fold {fold+1}: {e}\")\n",
        "            fold_mses.append(1e9) # Penalize with high error\n",
        "\n",
        "    # Calculate the average MSE across all folds\n",
        "    average_mse = np.mean(fold_mses)\n",
        "    print(f\"Average MSE for this set of hyperparameters: {average_mse:.4f}\")\n",
        "\n",
        "    # Return the average MSE (skopt minimizes this value)\n",
        "    return average_mse\n",
        "\n",
        "# --- 5. Run Bayesian Optimization ---\n",
        "print(\"\\n--- Starting Bayesian Optimization (This may take a while) ---\")\n",
        "results = gp_minimize(\n",
        "    func=objective_function,\n",
        "    dimensions=space,\n",
        "    n_calls=20, # Recommend starting small, then increase for production\n",
        "    n_random_starts=5,\n",
        "    random_state=42,\n",
        "    verbose=True,\n",
        "    n_jobs=1 # Set to 1 to avoid potential issues with TensorFlow multi-threading if not carefully managed\n",
        ")\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"\\n--- Bayesian Optimization Results ---\")\n",
        "print(f\"Best validation MSE found: {results.fun:.4f}\")\n",
        "print(\"Best hyperparameters:\")\n",
        "best_hyperparameters = {dim.name: value for dim, value in zip(space, results.x)}\n",
        "print(best_hyperparameters)"
      ],
      "metadata": {
        "id": "6f_ov8-vYDDU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}